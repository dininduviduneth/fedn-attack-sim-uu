{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c13d247-ddb8-4649-9ae0-f0ed9ab3ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedn import APIClient\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/fedn-attack-sim-uu/examples/mnist-pytorch')\n",
    "\n",
    "from combiner_config import COMBINER_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97fae7a7-215c-447f-a017-4eb430d86d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOVER_HOST = COMBINER_IP\n",
    "DISCOVER_PORT = 8092\n",
    "client = APIClient(DISCOVER_HOST, DISCOVER_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1336d28-0994-45e1-85b7-aa8f923d8078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: GBM001_25R_18_02\n",
      "1: GBM001_25R_19_01\n",
      "2: BASE_25R_20_00\n"
     ]
    }
   ],
   "source": [
    "sessions = [session['session_id'] for session in client.list_sessions()['result']]\n",
    "\n",
    "for i in range(len(sessions)):\n",
    "    print(f\"{i}: {sessions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902568be-cf27-4183-955f-4058f0327e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_ids_to_analyze = [int(x) for x in input(\"Provide a the sessions to be analyzed, seperated by a space: \").split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a514d-cc3e-4eec-bf6d-cc7eb5c5af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You chose the following sessions:\")\n",
    "[(num, session_id) for (num, session_id) in zip(session_ids_to_analyze, [sessions[i] for i in session_ids_to_analyze])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b433b78-bbce-4993-8a73-2c8c3289e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sets = [client.list_models(session) for session in [sessions[i] for i in session_ids_to_analyze]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d4d41-3460-4c4f-bfda-25a8881ded49",
   "metadata": {},
   "outputs": [],
   "source": [
    "[item['count'] for item in model_sets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de531a-76f0-4e7d-ab34-8ead1c48d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if round counts are same for all sessions\n",
    "const_model_count = model_sets[0]['count']\n",
    "for model_set in model_sets:\n",
    "    if model_set['count'] != const_model_count:\n",
    "        print(\"There is atleast one session which doesn't have the same model count\")\n",
    "        # Add code to terminate the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb1582-4b20-47e7-861e-d55c90ba10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_set in model_sets:\n",
    "    model_set['result'].reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27cc29-377e-4f11-8239-545ed993d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4ddb5-233f-4cc6-b816-c4777ddecfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.list_validations(modelId = '388a829d-f2db-42c5-9c3d-0c5278659999')\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for model_set in model_sets:\n",
    "    session_metrics = {\n",
    "        \"session_id\": model_set['result'][0]['session_id'],\n",
    "        \"training_loss\": [],\n",
    "        \"training_accuracy\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_accuracy\": []\n",
    "    }\n",
    "\n",
    "    model_set_results = model_set['result']\n",
    "\n",
    "    for model in model_set_results:\n",
    "        validations = client.list_validations(modelId = model['model'])\n",
    "        result_ids = [result_id for result_id in validations]\n",
    "        training_losses = []\n",
    "        training_accuracies = []\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        \n",
    "        for result_id in result_ids:\n",
    "            training_losses.append(json.loads(validations[result_id]['data'])['training_loss'])\n",
    "            training_accuracies.append(json.loads(validations[result_id]['data'])['training_accuracy'])\n",
    "            test_losses.append(json.loads(validations[result_id]['data'])['test_loss'])\n",
    "            test_accuracies.append(json.loads(validations[result_id]['data'])['test_accuracy'])\n",
    "\n",
    "        session_metrics['training_loss'].append(np.mean(training_losses))\n",
    "        session_metrics['training_accuracy'].append(np.mean(training_accuracies))\n",
    "        session_metrics['test_loss'].append(np.mean(test_losses))\n",
    "        session_metrics['test_accuracy'].append(np.mean(test_accuracies))\n",
    "\n",
    "    metrics.append(session_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404ed49-709a-4f4d-9def-b9711db58857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85de58-4467-4c15-91e8-32579b178898",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(const_model_count)\n",
    "\n",
    "for experiment in metrics:\n",
    "    plt.plot(x, experiment['training_loss'])\n",
    "\n",
    "plt.legend([metric['session_id'] for metric in metrics])\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('# of Rounds')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e9b04-f60e-4fce-8c90-7b8e41869e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(const_model_count)\n",
    "\n",
    "for experiment in metrics:\n",
    "    plt.plot(x, experiment['test_loss'])\n",
    "\n",
    "plt.legend([metric['session_id'] for metric in metrics])\n",
    "plt.title('Test Loss Comparison')\n",
    "plt.xlabel('# of Rounds')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c716313-d8cd-4cc9-a634-9b912304b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(const_model_count)\n",
    "\n",
    "for experiment in metrics:\n",
    "    plt.plot(x, experiment['training_accuracy'])\n",
    "\n",
    "plt.legend([metric['session_id'] for metric in metrics])\n",
    "plt.title('Training Accuracy Comparison')\n",
    "plt.xlabel('# of Rounds')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68c0a9c-4683-4ebf-8963-1a5812327ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(const_model_count)\n",
    "\n",
    "for experiment in metrics:\n",
    "    plt.plot(x, experiment['test_accuracy'])\n",
    "\n",
    "plt.legend([metric['session_id'] for metric in metrics])\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.xlabel('# of Rounds')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
